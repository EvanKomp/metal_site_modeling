{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test loading the dataset and collating it into batches for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. load a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = DatasetDict.load_from_disk(\"../data/dataset/metal_site_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = dd[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in ds.iter(batch_size=2):\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Collate a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "from typing import Dict, List, Optional\n",
    "import torch\n",
    "\n",
    "class AtomicSystemBatchCollator:\n",
    "    \"\"\"Processes batches of atomic systems with optional masking and noise.\n",
    "\n",
    "    Handles HuggingFace dataset batches (dict of lists) and converts to dict of tensors.\n",
    "    Optionally applies atom masking and position noise during training.\n",
    "\n",
    "    Args:\n",
    "        tokenizer: Tokenizer instance providing mask token and vocabulary\n",
    "        mask_rate: Fraction of atoms to mask during training, if None no masking\n",
    "        noise_rate: Fraction of positions to add noise, if None no noise\n",
    "        zero_noise_in_loss_rate: Additional positions to include in loss but not noise \n",
    "        noise_scale: Standard deviation of gaussian noise (Angstroms)\n",
    "        already_tokenized: If True, input is already tokenized, otherwise tokenizes\n",
    "        return_original_positions: If True, returns original positions before noise\n",
    "\n",
    "    Input batch format:\n",
    "        {\n",
    "            'atoms': List[List[int]], # Atomic number tokens\n",
    "            'atom_types': List[List[int]], # ATOM/HETATM tokens  \n",
    "            'positions': List[List[float]], # xyz coordinates\n",
    "            'id': List[str] # Optional system identifiers\n",
    "        }\n",
    "\n",
    "    Output format:\n",
    "        {\n",
    "            'atoms': [n_atoms_total] atom tokens\n",
    "            'atom_types': [n_atoms_total] record tokens\n",
    "            'positions': [n_atoms_total, 3] coordinates\n",
    "            'batch': [n_atoms_total] batch indices\n",
    "            'mask_mask': [n_atoms] mask of which atoms are masked\n",
    "            'noise_mask': [n_atoms] mask of which atoms are noised, if noising\n",
    "            'denoise_vectors': [n_atoms, 3] vectors required to denoise positions, if noising, for computing loss, this vector is scaled.\n",
    "            'noise_loss_mask': [n_atoms] mask of which atoms are used for loss, if noising\n",
    "            'id': [batch_size] original system IDs if provided\n",
    "            any other fields in input batch\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            tokenizer,\n",
    "            mask_rate: Optional[float] = None,\n",
    "            noise_rate: Optional[float] = None, \n",
    "            zero_noise_in_loss_rate: Optional[float] = None,\n",
    "            noise_scale: float = 1.0,\n",
    "            already_tokenized: bool = True,\n",
    "            return_original_positions: bool = False\n",
    "        ):\n",
    "        self.atom_mask_token = tokenizer.atom_mask_token\n",
    "        self.type_mask_token = tokenizer.type_mask_token\n",
    "        self.mask_rate = mask_rate\n",
    "        self.noise_rate = noise_rate\n",
    "        self.zero_noise_in_loss_rate = zero_noise_in_loss_rate\n",
    "        self.noise_scale = noise_scale\n",
    "        self.already_tokenized = already_tokenized\n",
    "        self.return_original_positions = return_original_positions\n",
    "\n",
    "\n",
    "    def __call__(self, batch: Dict[str, List]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Process a batch of atomic systems.\n",
    "\n",
    "        Args:\n",
    "            batch: HuggingFace format batch dictionary\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of processed tensors with optional masking/noise\n",
    "        \"\"\"\n",
    "        # Track batch sizes for creating index tensor\n",
    "        batch_sizes = [len(atoms) for atoms in batch['atoms']]\n",
    "        total_atoms = sum(batch_sizes)\n",
    "        logger.debug(f\"Processing batch with {len(batch_sizes)} systems, {total_atoms} total atoms\")\n",
    "\n",
    "        # Create batch index tensor\n",
    "        batch_idx = torch.repeat_interleave(\n",
    "            torch.arange(len(batch_sizes)), \n",
    "            torch.tensor(batch_sizes)\n",
    "        )\n",
    "\n",
    "        # tokenize if necessary\n",
    "        if not self.already_tokenized:\n",
    "            batch['atoms'] = [self.tokenizer.encode(x) for x in batch['atoms']]\n",
    "            batch['atom_types'] = [self.tokenizer.encode(x) for x in batch['atom_types']]\n",
    "\n",
    "        # Concatenate and convert to tensors\n",
    "        output = {\n",
    "            'atoms': torch.cat([torch.tensor(x) for x in batch['atoms']]),\n",
    "            'atom_types': torch.cat([torch.tensor(x) for x in batch['atom_types']]),\n",
    "            'positions': torch.cat([torch.tensor(x) for x in batch['positions']]),\n",
    "            'batch': batch_idx\n",
    "        }\n",
    "\n",
    "        # Apply masking\n",
    "        if self.mask_rate and self.mask_rate > 0:\n",
    "            n_mask = int(total_atoms * self.mask_rate)\n",
    "            mask_idx = torch.randperm(total_atoms)[:n_mask]\n",
    "            \n",
    "            output['atoms'][mask_idx] = self.atom_mask_token\n",
    "            output['atom_types'][mask_idx] = self.type_mask_token\n",
    "            output['mask_mask'] = torch.zeros(total_atoms, dtype=torch.bool)\n",
    "            output['mask_mask'][mask_idx] = True\n",
    "            \n",
    "            logger.debug(f\"Masked {n_mask} atoms\")\n",
    "\n",
    "        # Apply coordinate noise\n",
    "        if self.noise_rate and self.noise_rate > 0:\n",
    "            n_noise = int(total_atoms * self.noise_rate)\n",
    "            randperm = torch.randperm(total_atoms)\n",
    "            noise_idx = randperm[:n_noise]\n",
    "            \n",
    "            # Additional positions for loss but no noise\n",
    "            if self.zero_noise_in_loss_rate:\n",
    "                n_zero_noise = int(total_atoms * self.zero_noise_in_loss_rate)\n",
    "                zero_noise_idx = randperm[n_noise:n_noise+n_zero_noise]\n",
    "                noise_loss_idx = torch.cat([noise_idx, zero_noise_idx])\n",
    "            else:\n",
    "                noise_loss_idx = noise_idx\n",
    "\n",
    "\n",
    "            if self.return_original_positions:\n",
    "                output['original_positions'] = output['positions'].clone()\n",
    "\n",
    "            noise_vectors = torch.zeros_like(output['positions'])\n",
    "            noise_vectors[noise_idx] = torch.randn(n_noise, 3)\n",
    "            # move the atoms by noise times scale\n",
    "            # return vectors will not be scaled so model can be trained with low activations\n",
    "            output['positions'] = output['positions'] + noise_vectors * self.noise_scale\n",
    "            denoise_loss_vectors = noise_vectors * -1\n",
    "            output['denoise_vectors'] = denoise_loss_vectors\n",
    "            output['noise_loss_mask'] = torch.zeros(total_atoms, dtype=torch.bool)\n",
    "            output['noise_loss_mask'][noise_loss_idx] = True\n",
    "            output['noise_mask'] = torch.zeros(total_atoms, dtype=torch.bool)\n",
    "            output['noise_mask'][noise_idx] = True\n",
    "\n",
    "            logger.debug(f\"Added noise to {n_noise} positions, tracking loss on {len(noise_loss_idx)} positions\")\n",
    "        \n",
    "        # pass through other keys\n",
    "        for key, value in batch.items():\n",
    "            if key not in output:\n",
    "                output[key] = value\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metalsitenn.atom_vocabulary import AtomTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AtomTokenizer(\n",
    "    metal_known=False,\n",
    "    aggregate_uncommon=True,\n",
    "    keep_hydrogen=False,\n",
    "    allow_unknown=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = AtomicSystemBatchCollator(tokenizer=tokenizer, mask_rate=0.15, noise_rate=0.15, zero_noise_in_loss_rate=0.05, already_tokenized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:__main__:Processing batch with 2 systems, 434 total atoms\n",
      "DEBUG:__main__:Masked 65 atoms\n",
      "DEBUG:__main__:Added noise to 65 positions, tracking loss on 86 positions\n"
     ]
    }
   ],
   "source": [
    "out_batch = collator(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "atoms_out = out_batch['atoms']\n",
    "atom_types_out = out_batch['atom_types']\n",
    "positions_out = out_batch['positions']\n",
    "batch_out = out_batch['batch']\n",
    "mask_mask_out = out_batch['mask_mask']\n",
    "noise_mask_out = out_batch['noise_mask']\n",
    "denoise_vectors_out = out_batch['denoise_vectors']\n",
    "noise_loss_mask_out = out_batch['noise_loss_mask']\n",
    "id_out = out_batch['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6371, -0.3284, -0.4553],\n",
       "        [ 2.3044,  0.0064,  0.1806],\n",
       "        [ 1.3374,  0.9880,  0.3346],\n",
       "        [-0.8094, -0.3219, -0.7245],\n",
       "        [ 0.2485, -1.5461,  0.8510],\n",
       "        [ 0.4743, -0.1667,  1.4650],\n",
       "        [-0.3384,  0.9913, -0.0240],\n",
       "        [ 0.0853, -2.9400,  0.7277],\n",
       "        [-1.2542, -0.7127,  0.8286],\n",
       "        [ 0.5052,  2.0674, -2.2834],\n",
       "        [ 0.4355,  1.2461, -0.1633],\n",
       "        [ 0.6142, -2.0832, -2.0023],\n",
       "        [-1.2316, -0.1770,  0.4541],\n",
       "        [-1.1371, -0.4642,  0.6588],\n",
       "        [-1.5451,  0.3310,  1.2291],\n",
       "        [ 0.3362,  0.6862,  1.7811],\n",
       "        [-1.3093,  1.2934,  1.8894],\n",
       "        [ 0.4330,  1.0125,  0.2559],\n",
       "        [-0.0461, -0.1601, -0.4453],\n",
       "        [ 1.0123,  1.9930, -2.7784],\n",
       "        [-1.5821,  0.0900, -2.1105],\n",
       "        [ 2.3626, -0.3481,  1.5511],\n",
       "        [-0.5329, -0.3074,  2.0833],\n",
       "        [-1.6107,  0.2612, -0.1267],\n",
       "        [-0.4153, -0.6762, -0.7446],\n",
       "        [-0.2976, -0.8066,  0.5422],\n",
       "        [ 0.6019,  0.7598,  0.3473],\n",
       "        [ 0.1783,  1.7507, -0.7088],\n",
       "        [-2.7029, -0.5668, -0.9953],\n",
       "        [-1.2181, -0.3816, -0.9826],\n",
       "        [ 0.8510, -1.0308,  0.1735],\n",
       "        [-0.8616, -0.2068,  0.7223],\n",
       "        [-1.2562,  2.3502, -0.0904],\n",
       "        [-0.6653,  0.7654, -0.1105],\n",
       "        [-1.6169, -1.5757, -0.4678],\n",
       "        [ 0.3334,  0.2205,  1.1200],\n",
       "        [ 1.1929, -1.4388,  0.9150],\n",
       "        [ 0.0291, -0.6302, -1.9355],\n",
       "        [ 0.7694, -2.1310, -0.7928],\n",
       "        [-1.2203,  0.6147,  0.7727],\n",
       "        [-1.2727,  0.2594, -0.1443],\n",
       "        [-0.1124, -0.0688,  1.4260],\n",
       "        [-0.6091, -1.2815, -0.1988],\n",
       "        [-0.3056, -2.2190,  0.5209],\n",
       "        [-0.4060,  1.7457, -0.7798],\n",
       "        [-0.5304, -0.8180,  0.1458],\n",
       "        [ 0.3941,  0.0201,  0.6125],\n",
       "        [ 1.7302,  0.3238, -1.1584],\n",
       "        [-0.3188, -0.4085, -0.1547],\n",
       "        [ 0.1989, -1.4073, -1.1561],\n",
       "        [ 0.5792,  2.0015,  1.1706],\n",
       "        [ 1.8242,  0.9695, -1.7517],\n",
       "        [-1.4322, -0.2065, -0.7308],\n",
       "        [ 0.5212,  0.3922, -2.0926],\n",
       "        [ 0.4328,  0.4903, -1.4954],\n",
       "        [-0.3046,  1.6791, -0.4067],\n",
       "        [-1.1554,  0.2544,  1.6070],\n",
       "        [ 1.5569,  0.7651,  1.6019],\n",
       "        [ 0.1365,  0.5426, -0.0425],\n",
       "        [-1.2530, -1.3458,  1.5003],\n",
       "        [ 0.4384,  0.4408, -1.4788],\n",
       "        [ 0.6896, -0.6508, -0.8996],\n",
       "        [ 0.7960, -0.7282, -0.3438],\n",
       "        [ 0.6991,  0.1758,  0.4025],\n",
       "        [-0.7600,  0.9542,  0.1487]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denoise_vectors_out[noise_mask_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([86, 3])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denoise_vectors_out[noise_loss_mask_out].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([434])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atoms_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1958041958041958"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "56/286"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atoms_out[mask_mask_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9,  8,  8,  3,  8,  8,  3,  8,  3,  7,  7,  3,  3,  7,  3,  7,  3,  9,\n",
       "         8,  8,  8,  3,  3,  8,  3,  8,  3,  7,  3,  7,  3,  3,  7,  7,  3,  7,\n",
       "         3,  9,  8,  8,  3,  3,  3,  8,  3,  8,  3,  7,  3,  7,  3,  7,  7,  3,\n",
       "         7,  9,  8,  8,  8,  3,  3,  8,  3,  8,  3,  3,  7,  3,  7,  3,  7,  7,\n",
       "         3,  7,  3,  9,  8,  8,  8,  3,  3,  8,  3,  8,  3,  7,  3,  7,  3,  3,\n",
       "         8,  7,  3,  7,  7,  3,  9,  8,  8,  8,  3,  3,  8,  3,  8,  3,  8,  3,\n",
       "         7,  3,  7,  3,  3,  7,  7,  3,  7,  3,  8,  8,  3,  3,  8,  8,  3,  8,\n",
       "         3,  7,  3,  8,  7,  3,  8,  3,  3,  9,  8,  8,  8,  3,  3,  8,  3,  8,\n",
       "         3,  7,  3,  8,  7,  3,  7,  3,  3,  7,  3,  3,  7,  3,  3,  8,  3,  3,\n",
       "         3,  7,  3,  3,  8,  3,  3,  3,  7,  3,  7,  7, 12, 12,  8,  8,  8,  8,\n",
       "         8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  7,  3,  3,\n",
       "         8,  3,  3,  7,  3,  7,  7,  3,  3,  8,  3,  7,  3,  3,  8,  3,  3,  3,\n",
       "         7,  3,  3,  3,  3,  7,  3,  3,  7,  3,  3,  8,  3, 10,  7,  3,  3,  8,\n",
       "         7,  3,  3,  3,  7,  3,  3,  8, 10,  7,  3,  3,  3,  7,  7,  3,  3,  8,\n",
       "         3,  8,  3,  7,  3,  3,  8,  3,  3,  8,  3,  3,  3,  7,  3,  7,  7,  7,\n",
       "         3,  3,  8,  7,  3,  3,  8,  3,  3,  7,  3,  7,  7,  7,  3,  3,  8,  3,\n",
       "         3,  3,  8,  8,  7,  3,  3,  8,  3,  3,  7,  7,  3,  3,  8,  3,  3,  3,\n",
       "         3,  3,  3,  8,  7,  3,  3, 10,  7,  3,  3,  8,  3,  3,  7,  3,  3,  8,\n",
       "         3,  3,  7,  3,  7,  7,  7,  3,  3,  8,  7,  3,  3,  8,  3,  3,  3,  7,\n",
       "         3,  3,  8,  3,  3,  3,  7,  3,  8,  3,  3,  3,  7,  3,  7,  8,  8,  8,\n",
       "         8,  8,  8,  8,  8,  8,  8,  8,  8])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atoms_out[~mask_mask_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'atoms': ['<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>'],\n",
       " 'atom_types': ['<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>',\n",
       "  '<MASK>']}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(atoms=atoms_out[mask_mask_out], atom_types=atom_types_out[mask_mask_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9,  8,  0,  8,  0,  3,  8,  0,  8,  3,  8,  3,  7,  0,  7,  3,  3,  0,\n",
       "         7,  3,  7,  3,  9,  8,  8,  8,  3,  3,  8,  3,  8,  3,  0,  0,  7,  3,\n",
       "         7,  3,  3,  7,  7,  3,  7,  3,  9,  0,  8,  8,  3,  3,  0,  3,  8,  3,\n",
       "         8,  3,  7,  3,  7,  0,  3,  7,  7,  3,  7,  0,  9,  8,  8,  8,  3,  3,\n",
       "         8,  3,  8,  3,  0,  3,  7,  3,  7,  0,  3,  7,  7,  3,  7,  3,  9,  8,\n",
       "         8,  8,  3,  3,  0,  0,  8,  3,  8,  3,  7,  3,  7,  3,  3,  8,  7,  3,\n",
       "         7,  7,  3,  9,  8,  8,  8,  3,  3,  8,  3,  8,  3,  8,  3,  7,  3,  7,\n",
       "         3,  3,  7,  7,  3,  7,  3,  0,  8,  0,  8,  3,  3,  8,  0,  8,  3,  8,\n",
       "         3,  7,  3,  8,  7,  3,  8,  3,  3,  9,  8,  8,  8,  3,  3,  8,  3,  0,\n",
       "         0,  8,  3,  7,  3,  8,  7,  3,  7,  3,  3,  7,  3,  3,  0,  7,  3,  3,\n",
       "         8,  0,  3,  3,  3,  0,  7,  3,  3,  8,  3,  3,  3,  7,  3,  7,  7, 12,\n",
       "        12,  0,  8,  8,  0,  8,  8,  8,  8,  8,  8,  8,  8,  8,  0,  0,  8,  0,\n",
       "         8,  8,  8,  8,  8,  0,  8,  8,  7,  3,  3,  8,  3,  0,  3,  7,  3,  0,\n",
       "         7,  7,  3,  3,  8,  3,  0,  0,  0,  7,  3,  3,  8,  3,  3,  0,  3,  7,\n",
       "         0,  3,  3,  3,  3,  7,  3,  3,  0,  7,  3,  3,  8,  3, 10,  7,  3,  3,\n",
       "         8,  7,  3,  3,  0,  3,  7,  3,  3,  8,  0, 10,  7,  0,  0,  0,  3,  3,\n",
       "         0,  0,  3,  7,  7,  3,  3,  8,  3,  0,  0,  0,  8,  3,  0,  7,  3,  3,\n",
       "         8,  3,  0,  0,  3,  8,  3,  3,  3,  7,  3,  7,  7,  7,  3,  3,  8,  7,\n",
       "         3,  3,  8,  3,  0,  3,  7,  3,  7,  7,  7,  3,  3,  8,  3,  3,  3,  8,\n",
       "         8,  7,  3,  3,  8,  3,  3,  0,  7,  7,  3,  3,  8,  3,  3,  0,  3,  3,\n",
       "         3,  3,  8,  7,  0,  3,  0,  3, 10,  7,  3,  3,  8,  3,  0,  3,  7,  3,\n",
       "         3,  8,  3,  0,  3,  7,  3,  7,  7,  7,  3,  3,  8,  0,  7,  3,  3,  8,\n",
       "         0,  3,  3,  3,  7,  3,  3,  8,  0,  3,  3,  3,  7,  0,  3,  8,  3,  3,\n",
       "         3,  7,  3,  0,  7,  0,  8,  8,  0,  8,  8,  8,  8,  8,  8,  8,  0,  8,\n",
       "         8,  8])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atoms_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'atoms': ['C', 'C', 'CL', 'O'],\n",
       " 'atom_types': ['ATOM', 'ATOM', 'HETATM', 'ATOM']}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(**outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metalsitenn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
