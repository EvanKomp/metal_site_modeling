{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7723c7ff",
   "metadata": {},
   "source": [
    "# Creating neural net components based on node and 2d features\n",
    "\n",
    "General model architecture:\n",
    "- Accept positions, node features, edges and edge features (edge graph precomputed)\n",
    "- node features and edge features embedded into node and edge attributes (including distance)\n",
    "- node and edge hidden state updated with MLPs\n",
    "- Determinine l features due to eq. bond length, planar, and chirality constraints\n",
    "- Combine all into SO3 embeddings with scalars, l1 features, and map edge features onto all angular momenta but aligning the SO3 embeddings with the edge and then rotating after m=0 component has been computed\n",
    "- Attention layers on SO3 embeddings a la Equiformer, with a slight tweak: originally, only distance and optionally atom identies was used to attenuate attention weight. Here we use node and edge ATTRIBUTES concatenated along with distance.\n",
    "- Heads:\n",
    "  - Node classifier: Extract l=0 features and give to MLP\n",
    "  - Denoiser: One more SO3MLP laper and extract l=1 features, mean over the channels and convert from spherical harmonics to cartesian coordinates, outputting a vector\n",
    "  - Global classifier: TBD need a pooling mechanism\n",
    "\n",
    "\n",
    "## Backbone architecture\n",
    "\n",
    "0. Inputs are: \n",
    "  - atom and bond features $f_a, [N, ?]$\n",
    "  - positions $r, [N, 3]$\n",
    "  - edge features $f_e, [E, ?]$\n",
    "  - edge distances $R, [E,1]$\n",
    "\n",
    "1. Embed atom and bond features to node and edge attributes:\n",
    "  - $f_a' = Lin(Concat_{f_a^i}^{f_a}(Emb(f_a^i)))$\n",
    "  - $f_e' = Lin(Concat_{f_e^i}^{f_e}(Emb(f_e^i)))$\n",
    "2. Radial basis expansion of edge distances:\n",
    "  - $R' = RadialBasis(R)$\n",
    "3. Compute l1 vectors based on topology:\n",
    "  - $f_a^{l1} = get\\_gradients(topology, r), [N,3,3]$\n",
    "  - I don't think conversion to spherical harmonics is needed here as they will go into a linear layer\n",
    "4. Generate SO3 initial by:\n",
    "  - Node attributes $f_a^{l1}$ assigned to l=0 of SO3 embedding $h_a, [N, so3]$\n",
    "  - Incorporate initial edge information (distance RBF + bond features + initial atom embeddings):\n",
    "    - $h_a +=f(R', f_e', f_a'), [N,so3]$\n",
    "    - This is done by projecting edge attrs, src and dst attr, distance with eg. an MLP to represent the message, rotate the embeddings to the edge direction, assign this new hidden state to the m=0 component of the SO3 embedding, and then rotate the whole embedding to the edge direction.\n",
    "  - Mix the l=1 features at this point with the 3 features from topology:\n",
    "    - $h_a[:, 1:4, :] = mix(h_a[:, 1:4, :], f_a^{l1})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "8fcab8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6a04e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CHANNELS = 64\n",
    "LMAX_LIST = [3]\n",
    "MMAX_LIST = [2]\n",
    "NUM_HEADS = 4\n",
    "\n",
    "NUM_RESOLUTIONS = len(LMAX_LIST)\n",
    "SPHERE_CHANNELS_ALL = NUM_RESOLUTIONS * NUM_CHANNELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec073ba",
   "metadata": {},
   "source": [
    "### 0. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "616b4d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metalsitenn.dataloading import MetalSiteDataset\n",
    "from metalsitenn.featurizer import MetalSiteCollator\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aa942911",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = MetalSiteDataset(\n",
    "    cache_folder='../../bonnanzio_metal_site_modeling/data/1/1.1_parse_sites_metadata',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b16476bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = MetalSiteCollator(\n",
    "    atom_features=['element', 'charge', 'nhyd', 'hyb'],\n",
    "    bond_features=['bond_order', 'is_in_ring', 'is_aromatic'],\n",
    "    metal_unknown=False,\n",
    "    metal_classification=True,\n",
    "    residue_collapse_do=True,\n",
    "    residue_collapse_time=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "28ce0475",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(\n",
    "    ds,\n",
    "    batch_size=4,\n",
    "    collate_fn=collator,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e42d8900",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d020ba42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'element': 46,\n",
       " 'charge': 8,\n",
       " 'nhyd': 7,\n",
       " 'hyb': 8,\n",
       " 'bond_order': 6,\n",
       " 'is_in_ring': 3,\n",
       " 'is_aromatic': 3}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_vocab_sizes = collator.featurizer.get_feature_vocab_sizes()\n",
    "feature_vocab_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d373ab0",
   "metadata": {},
   "source": [
    "### 0.5 - Init some equivariant related classes necessary throughout the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "80f27739",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairchem.core.models.equiformer_v2.so3 import (\n",
    "    CoefficientMappingModule,\n",
    "    SO3_Embedding,\n",
    "    SO3_Grid,\n",
    "    SO3_LinearV2,\n",
    "    SO3_Rotation,\n",
    ")\n",
    "from fairchem.core.models.equiformer_v2.layer_norm import get_normalization_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6d0eab65",
   "metadata": {},
   "outputs": [],
   "source": [
    "mappingReduced = CoefficientMappingModule(\n",
    "    lmax_list=LMAX_LIST,\n",
    "    mmax_list=MMAX_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "359b7eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SO3_rotation = nn.ModuleList()\n",
    "for i in range(NUM_RESOLUTIONS):\n",
    "    SO3_rotation.append(SO3_Rotation(LMAX_LIST[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4cb6e370",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_pos = batch.positions[batch.edge_index[:,0]]\n",
    "dst_pos = batch.positions[batch.edge_index[:,1]]\n",
    "edge_distance_vector = dst_pos - src_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2cc6f851",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairchem.core.models.equiformer_v2.edge_rot_mat import init_edge_rot_mat\n",
    "edge_rot_mat = init_edge_rot_mat(edge_distance_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d3a1c398",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sor_rot in SO3_rotation:\n",
    "    sor_rot.set_wigner(edge_rot_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "1944bb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairchem.core.models.equiformer_v2.module_list import ModuleListInfo\n",
    "\n",
    "SO3_grid = ModuleListInfo(\n",
    "    f\"({max(LMAX_LIST)}, {max(LMAX_LIST)})\"\n",
    ")\n",
    "for lval in range(max(LMAX_LIST) + 1):\n",
    "    SO3_m_grid = nn.ModuleList()\n",
    "    for m in range(max(LMAX_LIST) + 1):\n",
    "        SO3_m_grid.append(\n",
    "            SO3_Grid(\n",
    "                lval,\n",
    "                m,\n",
    "                resolution=None,\n",
    "                normalization=\"component\",\n",
    "            )\n",
    "        )\n",
    "    SO3_grid.append(SO3_m_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e10a83",
   "metadata": {},
   "source": [
    "### 1. Node and edge embeddings into attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014ac505",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeEmbedder(nn.Module):\n",
    "    \"\"\"\n",
    "    Embed atom features, concat, then project to output dimension.\n",
    "    \n",
    "    Handles all atom-level features in the molecular graph.\n",
    "    \n",
    "    Args:\n",
    "        feature_vocab_sizes: Dict mapping atom feature names to vocab sizes\n",
    "        atom_features: List of atom feature names\n",
    "        output_dim: Output dimension for concatenated atom features\n",
    "        embedding_dim: Individual embedding dimension per feature\n",
    "        use_bias: Whether to use bias in final projection layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_vocab_sizes: Dict[str, int],\n",
    "        atom_features: List[str],\n",
    "        output_dim: int = 64,\n",
    "        embedding_dim: int = 32,\n",
    "        use_bias: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.atom_features = atom_features\n",
    "        \n",
    "        # Separate embeddings for each atom feature\n",
    "        self.embeddings = nn.ModuleDict()\n",
    "        for feature in atom_features:\n",
    "            vocab_size = feature_vocab_sizes[feature]\n",
    "            self.embeddings[feature] = nn.Embedding(vocab_size, embedding_dim)\n",
    "            \n",
    "        # Project concatenated features to desired output dimension\n",
    "        concat_dim = len(atom_features) * embedding_dim\n",
    "        self.projection = nn.Linear(concat_dim, output_dim, bias=use_bias)\n",
    "        \n",
    "    def forward(self, feature_dict: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Embed and project atom features.\n",
    "        \n",
    "        Args:\n",
    "            feature_dict: Dict of feature_name -> token_indices tensor\n",
    "            \n",
    "        Returns:\n",
    "            Atom embeddings tensor of shape (num_atoms, output_dim)\n",
    "        \"\"\"\n",
    "        # Embed each atom feature individually\n",
    "        embedded_features = []\n",
    "        for name in self.atom_features:\n",
    "            tokens = feature_dict[name]\n",
    "            embedded = self.embeddings[name](tokens.squeeze(-1))\n",
    "            embedded_features.append(embedded)\n",
    "            \n",
    "        # Concatenate all atom features\n",
    "        atom_embeds = torch.cat(embedded_features, dim=-1)\n",
    "        \n",
    "        # Project to output dimension\n",
    "        atom_embeds = self.projection(atom_embeds)\n",
    "        return atom_embeds\n",
    "\n",
    "\n",
    "class EdgeEmbedder(nn.Module):\n",
    "    \"\"\"\n",
    "    Embed bond features, concat, then project to output dimension.\n",
    "    \n",
    "    Handles all edge-level features in the molecular graph.\n",
    "    \n",
    "    Args:\n",
    "        feature_vocab_sizes: Dict mapping bond feature names to vocab sizes\n",
    "        bond_features: List of bond feature names\n",
    "        output_dim: Output dimension for concatenated bond features\n",
    "        embedding_dim: Individual embedding dimension per feature\n",
    "        use_bias: Whether to use bias in final projection layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_vocab_sizes: Dict[str, int],\n",
    "        bond_features: List[str],\n",
    "        output_dim: int = 64,\n",
    "        embedding_dim: int = 32,\n",
    "        use_bias: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.bond_features = bond_features\n",
    "        \n",
    "        # Separate embeddings for each bond feature\n",
    "        self.embeddings = nn.ModuleDict()\n",
    "        for feature in bond_features:\n",
    "            vocab_size = feature_vocab_sizes[feature]\n",
    "            self.embeddings[feature] = nn.Embedding(vocab_size, embedding_dim)\n",
    "            \n",
    "        # Project concatenated features to desired output dimension\n",
    "        concat_dim = len(bond_features) * embedding_dim\n",
    "        self.projection = nn.Linear(concat_dim, output_dim, bias=use_bias)\n",
    "        \n",
    "    def forward(self, feature_dict: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Embed and project bond features.\n",
    "        \n",
    "        Args:\n",
    "            feature_dict: Dict of feature_name -> token_indices tensor\n",
    "            \n",
    "        Returns:\n",
    "            Bond embeddings tensor of shape (num_bonds, output_dim)\n",
    "        \"\"\"\n",
    "        # Embed each bond feature individually\n",
    "        embedded_features = []\n",
    "        for name in self.bond_features:\n",
    "            tokens = feature_dict[name]\n",
    "            embedded = self.embeddings[name](tokens.squeeze(-1))\n",
    "            embedded_features.append(embedded)\n",
    "            \n",
    "        # Concatenate all bond features\n",
    "        bond_embeds = torch.cat(embedded_features, dim=-1)\n",
    "        \n",
    "        # Project to output dimension\n",
    "        bond_embeds = self.projection(bond_embeds)\n",
    "        return bond_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5373c92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "l0nodeembedder = NodeEmbedder(\n",
    "    feature_vocab_sizes=feature_vocab_sizes,\n",
    "    atom_features=['element', 'charge', 'nhyd', 'hyb'],\n",
    "    output_dim=SPHERE_CHANNELS_ALL,\n",
    "    embedding_dim=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "38c2ad8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dict = {}\n",
    "for feature in collator.featurizer.atom_features:\n",
    "    if getattr(batch, feature) is not None:\n",
    "        feature_dict[feature] = getattr(batch, feature)\n",
    "    else:\n",
    "        raise ValueError(f\"Feature {feature} not found in batch\")\n",
    "\n",
    "for feature in collator.featurizer.bond_features:\n",
    "    if getattr(batch, feature) is not None:\n",
    "        feature_dict[feature] = getattr(batch, feature)\n",
    "    else:\n",
    "        raise ValueError(f\"Feature {feature} not found in batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4ff0059c",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_attributes = l0nodeembedder(feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42d9de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node attributes shape: torch.Size([362, 64])\n",
      "Edge attributes shape: torch.Size([7240, 64])\n"
     ]
    }
   ],
   "source": [
    "print(\"Node attributes shape:\", node_attributes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a91599",
   "metadata": {},
   "source": [
    "### 2. Radial basis expansion of edge distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ad023ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairchem.core.models.scn.smearing import GaussianSmearing\n",
    "start = 0.0\n",
    "stop = 7.0\n",
    "num_basis = 24\n",
    "\n",
    "radial_basis = GaussianSmearing(\n",
    "    start=0.0,\n",
    "    stop=7.0,\n",
    "    num_gaussians=24,\n",
    "    basis_width_scalar=(stop - start) / num_basis\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "982e7768",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = radial_basis(batch.distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9c7b3daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance RBF shape: torch.Size([7240, 24])\n"
     ]
    }
   ],
   "source": [
    "print(\"Distance RBF shape:\", R.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f675aceb",
   "metadata": {},
   "source": [
    "### 3. Compute the l1 feature vectors based on topology "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "74a7ed78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metalsitenn.placer_modules.losses import bondLoss\n",
    "from metalsitenn.placer_modules.geometry import triple_prod\n",
    "\n",
    "def compute_positional_topology_gradients(\n",
    "    r: torch.Tensor,\n",
    "    bond_indexes: torch.Tensor,\n",
    "    bond_lengths: torch.Tensor,\n",
    "    chirals: torch.Tensor,\n",
    "    planars: torch.Tensor,\n",
    "    gclip: float = 100.0,\n",
    "    atom_mask: Optional[torch.Tensor] = None,\n",
    "):\n",
    "    \"\"\"Get gradients of positions with respect to topology features.\n",
    "\n",
    "    A la. ChemNet ; https://github.com/baker-laboratory/PLACER/blob/main/modules/model.py\n",
    "\n",
    "    Some additions:\n",
    "    - the gradient is flipped in direction such that it makes physical sense - these vectors point in the direction the atom should\n",
    "      move. This should make no difference for downstream neural operations as weights can flip anyway.\n",
    "    - option to provide mask for atoms, which will zero out gradients for masked atoms. This is useful for training with masked atoms.\n",
    "\n",
    "    Args:\n",
    "        r (torch.Tensor): Atom positions, shape (N, 3).\n",
    "        bonds (torch.Tensor): Bond indexes, shape (M, 2).\n",
    "        bond_lengths (torch.Tensor): Bond lengths, shape (M,1).\n",
    "        chirals (torch.Tensor): Chirality features, shape (O,5).\n",
    "        planars (torch.Tensor): Planarity features, shape (P,5).\n",
    "        gclip (float): Gradient clipping value.\n",
    "        atom_mask (torch.Tensor, optional): Mask for atoms, shape (N,). If provided, gradients will be zeroed for masked atoms.\n",
    "\n",
    "    Returns:\n",
    "        grads (torch.Tensor): Gradients of shape (N, 3, 3). (vectors from each of both length, chirals, planars).\n",
    "    \"\"\"\n",
    "    N = r.shape[0]\n",
    "    device = r.device\n",
    "\n",
    "    with torch.enable_grad():\n",
    "        r_detached = r.detach() # so that the computation graph does not include the result of this function, which is essentially external context / input\n",
    "        r_detached.requires_grad = True  # Enable gradients for positions\n",
    "\n",
    "        g = torch.zeros((N, 3, 3), device=device)\n",
    "    \n",
    "        # Compute bond gradients\n",
    "        if len(bond_indexes) > 0:\n",
    "            l = bondLoss(\n",
    "                r_detached,\n",
    "                ij=bond_indexes,\n",
    "                b0=bond_lengths,\n",
    "                mean=False\n",
    "            )\n",
    "            g[:, 0] = torch.autograd.grad(l, r_detached)[0].data\n",
    "\n",
    "        # Compute chirality gradients\n",
    "        if len(chirals) > 0:\n",
    "            o,i,j,k = r_detached[chirals].permute(1, 0, 2)\n",
    "            l = ((triple_prod(o-i,o-j,o-k,norm=True)-0.70710678)**2).sum()\n",
    "            g[:, 1] = torch.autograd.grad(l, r_detached)[0].data\n",
    "\n",
    "        # Compute planarity gradients\n",
    "        if len(planars) > 0:\n",
    "            o,i,j,k = r_detached[planars].permute(1, 0, 2)\n",
    "            l = ((triple_prod(o-i,o-j,o-k,norm=True)**2).sum())\n",
    "            g[:, 2] = torch.autograd.grad(l, r_detached)[0].data\n",
    "\n",
    "        # Scale and clip\n",
    "        g = torch.nan_to_num(g, nan=0.0, posinf=gclip, neginf=-gclip)\n",
    "        gnorm = torch.linalg.norm(g, dim=-1)\n",
    "        mask = gnorm > gclip\n",
    "        g[mask] /= gnorm[mask][...,None]\n",
    "        g[mask]  *= gclip\n",
    "\n",
    "        # flip direction of gradients\n",
    "        g = -g\n",
    "\n",
    "        # Zero gradients for masked atoms\n",
    "        if atom_mask is not None:\n",
    "            g *= atom_mask[:, None, None].to(g.dtype)\n",
    "\n",
    "        return g.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ee0ca6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure to mask out any masked atoms\n",
    "masked_elements = batch.element == collator.featurizer.tokenizers['element'].mask_token_id\n",
    "if sum(masked_elements) == 0:\n",
    "    atom_mask = None\n",
    "else:\n",
    "    atom_mask = ~masked_elements\n",
    "\n",
    "node_l1_gradients = compute_positional_topology_gradients(\n",
    "    r=batch.positions,\n",
    "    bond_indexes=batch.topology['bonds'],\n",
    "    bond_lengths=batch.topology['bond_lengths'],\n",
    "    chirals=batch.topology['chirals'],\n",
    "    planars=batch.topology['planars'],\n",
    "    gclip=100.0,\n",
    "    atom_mask=atom_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bf37ce10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node l1 gradients shape: torch.Size([362, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "print(\"Node l1 gradients shape:\", node_l1_gradients.shape)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3f296f",
   "metadata": {},
   "source": [
    "Note that the third dimension here is channels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97644f8",
   "metadata": {},
   "source": [
    "### 4. Generate SO3 initial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e10fb2",
   "metadata": {},
   "source": [
    "#### First just send the node attributes onto the l=0 component of the SO3 embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6574a48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairchem.core.models.equiformer_v2.so3 import SO3_Embedding, SO3_LinearV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ed92797c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SO3ScalarEmbedder(nn.Module):\n",
    "    \"\"\"\n",
    "    Converts pre-computed atom embeddings to SO3 embeddings by projecting them \n",
    "    to the l=0, m=0 coefficients across multiple resolutions.\n",
    "    \n",
    "    Args:\n",
    "        input_dim (int): Dimension of input atom embeddings\n",
    "        lmax_list (list[int]): List of maximum degrees (l) for each resolution\n",
    "        sphere_channels (int): Number of spherical channels per resolution\n",
    "        device (str): Device to place tensors on\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        lmax_list: list[int],\n",
    "        sphere_channels: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lmax_list = lmax_list\n",
    "        self.sphere_channels = sphere_channels\n",
    "        self.num_resolutions = len(lmax_list)\n",
    "        self.sphere_channels_all = self.num_resolutions * sphere_channels\n",
    "\n",
    "    def forward(self, atom_embeddings: torch.Tensor) -> SO3_Embedding:\n",
    "        \"\"\"\n",
    "        Convert atom embeddings to SO3 embeddings.\n",
    "        \n",
    "        Args:\n",
    "            atom_embeddings (torch.Tensor): Input atom embeddings of shape (N, sphere_channels_all)\n",
    "            \n",
    "        Returns:\n",
    "            SO3_Embedding: SO3 embedding with l=0, m=0 coefficients initialized\n",
    "        \"\"\"\n",
    "        num_atoms = atom_embeddings.shape[0]\n",
    "        if atom_embeddings.shape[1] != self.sphere_channels_all:\n",
    "            raise ValueError(\n",
    "                f\"Expected atom_embeddings shape (N, {self.sphere_channels_all}), \"\n",
    "                f\"but got {atom_embeddings.shape}\"\n",
    "            )\n",
    "        \n",
    "        # Initialize SO3 embedding\n",
    "        x = SO3_Embedding(\n",
    "            num_atoms,\n",
    "            self.lmax_list,\n",
    "            self.sphere_channels,\n",
    "            atom_embeddings.device,\n",
    "            atom_embeddings.dtype,\n",
    "        )\n",
    "        \n",
    "        # Fill in the l=0, m=0 coefficients for each resolution\n",
    "        offset_res = 0  # Offset in SO3 embedding coefficient dimension\n",
    "        offset_channels = 0  # Offset in projected embedding channels\n",
    "        \n",
    "        for i in range(self.num_resolutions):\n",
    "            if self.num_resolutions == 1:\n",
    "                # Single resolution case - use all projected channels\n",
    "                x.embedding[:, offset_res, :] = atom_embeddings\n",
    "            else:\n",
    "                # Multi-resolution case - split channels across resolutions\n",
    "                x.embedding[:, offset_res, :] = atom_embeddings[\n",
    "                    :, offset_channels : offset_channels + self.sphere_channels\n",
    "                ]\n",
    "            \n",
    "            # Update offsets for next resolution\n",
    "            offset_channels += self.sphere_channels\n",
    "            offset_res += int((self.lmax_list[i] + 1) ** 2)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "710ce2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_scaler_embedder = SO3ScalarEmbedder(\n",
    "    lmax_list=LMAX_LIST,\n",
    "    sphere_channels=NUM_CHANNELS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b53fd5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_a = node_scaler_embedder(node_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f99e7f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(h_a.embedding[:,0] == node_attributes).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae983cc0",
   "metadata": {},
   "source": [
    "#### Generate edge embeddings and assign them to the m=0 component of the SO3 embedding\n",
    "\n",
    "##### First a general method for taking distance, node AND edge attributes and projecting them to target size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "dccbd19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairchem.core.models.equiformer_v2.radial_function import RadialFunction \n",
    "\n",
    "class EdgeProjector(nn.Module):\n",
    "    \"\"\"\n",
    "    Embed edges to output of target size.\n",
    "\n",
    "    In equiformer, the radial basis distance is optionally combined with a src and dst node embedding,\n",
    "    then projected with an MLP \"radial_func\" to a target size - the target size depends on the application.\n",
    "    This class is meant to extract that functionality out and allow us to use all the extra node and edge features we\n",
    "    have beyond just the distance and atomic identity.\n",
    "\n",
    "    Args:\n",
    "        radial_basis_size (int): Size of RBF expected\n",
    "        feature_vocab_sizes (Dict[str, int]): Dictionary mapping feature names to vocab sizes\n",
    "        use_edge_features (bool): Whether to use edge features\n",
    "        bond_features (List[str]): List of bond feature names to use\n",
    "        use_node_features (bool): Whether to use node features\n",
    "        node_features (List[str]): List of node feature names to use\n",
    "        output_dim (int): Output dimension for the edge embeddings\n",
    "        embedding_dim (int): Embedding dimension for node and edge features from NodeEmbedder and EdgeEmbedder\n",
    "        embedding_use_bias (bool): Whether to use bias in the embedding layers\n",
    "        projector_hidden_layers (int): Number of hidden layers in the projector Radial func\n",
    "        projector_output_size (int): Output size of the projector Radial func\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        radial_basis_size: int,\n",
    "        feature_vocab_sizes: Dict[str, int]={},\n",
    "        use_edge_features: bool=True,\n",
    "        bond_features: List[str]=['bond_order', 'is_in_ring', 'is_aromatic'],\n",
    "        use_node_features: bool=True,\n",
    "        node_features: List[str]=['element', 'charge', 'nhyd', 'hyb'],\n",
    "        output_dim: int = 64,\n",
    "        embedding_dim: int = 32,\n",
    "        embedding_use_bias: bool = True,\n",
    "        use_projector: bool = True,\n",
    "        projector_hidden_layers: int = 1,\n",
    "        projector_size: int = 64\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.radial_basis_size = radial_basis_size\n",
    "        self.feature_vocab_sizes = feature_vocab_sizes\n",
    "        self.use_edge_features = use_edge_features\n",
    "        self.bond_features = bond_features\n",
    "        self.use_node_features = use_node_features\n",
    "        self.node_features = node_features\n",
    "        self.output_dim = output_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.use_bias = embedding_use_bias\n",
    "        self.projector_hidden_layers = projector_hidden_layers\n",
    "        self.projector_size = projector_size\n",
    "        self.use_projector = use_projector\n",
    "\n",
    "        # if we are using node features, create embedders\n",
    "        if self.use_node_features:\n",
    "            self.source_embedding = NodeEmbedder(\n",
    "                feature_vocab_sizes=self.feature_vocab_sizes,\n",
    "                atom_features=self.node_features,\n",
    "                output_dim=embedding_dim,\n",
    "                embedding_dim=embedding_dim,\n",
    "                use_bias=embedding_use_bias\n",
    "            )\n",
    "            self.destination_embedding = NodeEmbedder(\n",
    "                feature_vocab_sizes=self.feature_vocab_sizes,\n",
    "                atom_features=self.node_features,\n",
    "                output_dim=embedding_dim,\n",
    "                embedding_dim=embedding_dim,\n",
    "                use_bias=embedding_use_bias\n",
    "            )\n",
    "        else:\n",
    "            self.source_embedding = None\n",
    "            self.destination_embedding = None\n",
    "\n",
    "        # if we are using edge features, create embedder\n",
    "        if self.use_edge_features:\n",
    "            self.edge_embedding = EdgeEmbedder(\n",
    "                feature_vocab_sizes=self.feature_vocab_sizes,\n",
    "                bond_features=self.bond_features,\n",
    "                output_dim=embedding_dim,\n",
    "                embedding_dim=embedding_dim,\n",
    "                use_bias=embedding_use_bias\n",
    "            )\n",
    "        else:\n",
    "            self.edge_embedding = None\n",
    "\n",
    "        # get the epected input size for the radial function\n",
    "        input_size = radial_basis_size\n",
    "        if self.use_edge_features:\n",
    "            input_size += embedding_dim\n",
    "        if self.use_node_features:\n",
    "            input_size += 2 * embedding_dim\n",
    "\n",
    "        self.input_size = input_size\n",
    "\n",
    "        # radial function to project the input to the output dimension\n",
    "        if self.use_projector:\n",
    "            channels_list = [input_size] + [self.projector_size] * self.projector_hidden_layers + [self.output_dim]\n",
    "            self.radial_func = RadialFunction(\n",
    "                channels_list=channels_list,\n",
    "            )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        R: torch.Tensor, # [E, radial_basis_size]\n",
    "        edge_index: torch.Tensor, # [E,2]\n",
    "        feature_dict: Dict[str, torch.Tensor]={},\n",
    "    ):\n",
    "        to_concat = []\n",
    "        # radial basis distance\n",
    "        to_concat.append(R)\n",
    "\n",
    "        # edge features\n",
    "        if self.use_edge_features:\n",
    "            edge_features = self.edge_embedding(feature_dict)\n",
    "            to_concat.append(edge_features)\n",
    "\n",
    "        # node features\n",
    "        if self.use_node_features:\n",
    "            nodes_embedded = self.source_embedding(feature_dict)\n",
    "\n",
    "            # Extract the source and destination node embeddings\n",
    "            src_embeddings = nodes_embedded[edge_index[:, 0]]\n",
    "            dst_embeddings = nodes_embedded[edge_index[:, 1]]\n",
    "            # Concatenate source and destination node embeddings\n",
    "            to_concat.append(src_embeddings)\n",
    "            to_concat.append(dst_embeddings)\n",
    "\n",
    "        # concatenate all features\n",
    "        concatenated = torch.cat(to_concat, dim=-1)\n",
    "        if self.use_projector:\n",
    "            # pass through radial function\n",
    "            output = self.radial_func(concatenated)\n",
    "\n",
    "            return output\n",
    "        else:\n",
    "            return concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0daae1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a quick test of the class, we do not know the required output size yet\n",
    "edge_projector = EdgeProjector(\n",
    "    radial_basis_size=R.shape[1],\n",
    "    feature_vocab_sizes=feature_vocab_sizes,\n",
    "    use_edge_features=True,\n",
    "    bond_features=collator.featurizer.bond_features,\n",
    "    use_node_features=True,\n",
    "    node_features=collator.featurizer.atom_features,\n",
    "    output_dim=5,\n",
    "    embedding_dim=32,\n",
    "    embedding_use_bias=True,\n",
    "    projector_hidden_layers=1,\n",
    "    projector_size=64\n",
    ")\n",
    "unusable_edge_features = edge_projector(\n",
    "    R,\n",
    "    edge_index=batch.edge_index,\n",
    "    feature_dict=feature_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159991c8",
   "metadata": {},
   "source": [
    "##### Now we use it inside a modified EdgeDegreeEmbedding class from eqf2\n",
    "https://github.com/facebookresearch/fairchem/blob/977a80328f2be44649b414a9907a1d6ef2f81e95/src/fairchem/core/models/equiformer_v2/input_block.py#L12\n",
    "\n",
    "Difference is here we have additional information for our invariant edge features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf95df62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeDegreeEmbedding(torch.nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        sphere_channels (int):      Number of spherical channels\n",
    "\n",
    "        lmax_list (list:int):       List of degrees (l) for each resolution\n",
    "        mmax_list (list:int):       List of orders (m) for each resolution\n",
    "\n",
    "        SO3_rotation (list:SO3_Rotation): Class to calculate Wigner-D matrices and rotate embeddings\n",
    "        mappingReduced (CoefficientMappingModule): Class to convert l and m indices once node embedding is rotated\n",
    "\n",
    "        DEPRECATED, using EdgeProjector instead\n",
    "        # max_num_elements (int):     Maximum number of atomic numbers\n",
    "        # edge_channels_list (list:int):  List of sizes of invariant edge embedding. For example, [input_channels, hidden_channels, hidden_channels].\n",
    "                                        The last one will be used as hidden size when `use_atom_edge_embedding` is `True`.\n",
    "        # use_atom_edge_embedding (bool): Whether to use atomic embedding along with relative distance for edge scalar features\n",
    "        radial_basis_size (int):     Number of radial basis functions expected\n",
    "        feature_vocab_sizes (list:int): List of sizes of feature vocabularies\n",
    "        use_edge_features (bool):    Whether to use edge features\n",
    "        bond_features (list:str): List of bond feature names to use if using any\n",
    "        use_node_features (bool): Whether to use node features\n",
    "        node_features (list:str): List of node feature names to use if using any\n",
    "        embedding_dim (int):        Embedding dimension for node and edge features\n",
    "        embedding_use_bias (bool):  Whether to use bias in the embedding layers\n",
    "        projector_hidden_layers (int): Number of hidden layers in the projector Radial func\n",
    "        projector_size (int):       Hidden layer size of the projector Radial func\n",
    "        NOTE: Output size of radial func is determined by number of m0 coefficients available.\n",
    "\n",
    "        rescale_factor (float):     Rescale the sum aggregation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sphere_channels: int,\n",
    "        lmax_list: list[int],\n",
    "        mmax_list: list[int],\n",
    "        SO3_rotation,\n",
    "        mappingReduced,\n",
    "        radial_basis_size: int,\n",
    "        feature_vocab_sizes: Dict[str, int]={},\n",
    "        use_edge_features: bool=True,\n",
    "        bond_features: List[str]=['bond_order', 'is_in_ring', 'is_aromatic'],\n",
    "        use_node_features: bool=True,\n",
    "        node_features: List[str]=['atomic_number', 'formal_charge'],\n",
    "        embedding_dim: int=128,\n",
    "        embedding_use_bias: bool=True,\n",
    "        projector_hidden_layers: int=2,\n",
    "        projector_size: int=64,\n",
    "        rescale_factor: float=1.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.sphere_channels = sphere_channels\n",
    "        self.lmax_list = lmax_list\n",
    "        self.mmax_list = mmax_list\n",
    "        self.num_resolutions = len(self.lmax_list)\n",
    "        self.SO3_rotation = SO3_rotation\n",
    "        self.mappingReduced = mappingReduced\n",
    "\n",
    "        self.m_0_num_coefficients: int = self.mappingReduced.m_size[0]\n",
    "        self.m_all_num_coefficents: int = len(self.mappingReduced.l_harmonic)\n",
    "\n",
    "        # output size as\n",
    "        rad_output_size = self.m_0_num_coefficients * self.sphere_channels\n",
    "        self.rad_func = EdgeProjector(\n",
    "            radial_basis_size=radial_basis_size,\n",
    "            feature_vocab_sizes=feature_vocab_sizes,\n",
    "            use_edge_features=use_edge_features,\n",
    "            bond_features=bond_features,\n",
    "            use_node_features=use_node_features,\n",
    "            node_features=node_features,\n",
    "            output_dim=rad_output_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "            embedding_use_bias=embedding_use_bias,\n",
    "            projector_hidden_layers=projector_hidden_layers,\n",
    "            projector_size=projector_size\n",
    "        )\n",
    "\n",
    "        self.rescale_factor = rescale_factor\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        edge_distance_rbf: torch.Tensor,\n",
    "        edge_index: torch.Tensor, \n",
    "        num_nodes: int, \n",
    "        feature_dict: Dict[str, torch.Tensor] = {},\n",
    "        node_offset: int = 0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Forward pass for edge degree embedding.\n",
    "        \n",
    "        Args:\n",
    "            edge_distance_rbf (torch.Tensor): Radial basis function expansion of edge distances [E, radial_basis_size]\n",
    "            edge_index (torch.Tensor): Edge indices [2, E]\n",
    "            num_nodes (int): Number of nodes in the graph\n",
    "            feature_dict (Dict[str, torch.Tensor]): Dictionary containing node and edge features\n",
    "            node_offset (int): Offset for node indices (default: 0)\n",
    "            \n",
    "        Returns:\n",
    "            SO3_Embedding: Edge embedding in SO3 format\n",
    "        \"\"\"\n",
    "        # Use EdgeProjector to compute edge features including distance, node features, and edge features\n",
    "        x_edge_m_0 = self.rad_func(edge_distance_rbf, edge_index, feature_dict)\n",
    "        \n",
    "        # Reshape to [num_edges, m_0_coefficients, sphere_channels]\n",
    "        x_edge_m_0 = x_edge_m_0.reshape(\n",
    "            -1, self.m_0_num_coefficients, self.sphere_channels\n",
    "        )\n",
    "        \n",
    "        # Pad with zeros for higher m coefficients\n",
    "        x_edge_m_pad = torch.zeros(\n",
    "            (\n",
    "                x_edge_m_0.shape[0],\n",
    "                (self.m_all_num_coefficents - self.m_0_num_coefficients),\n",
    "                self.sphere_channels,\n",
    "            ),\n",
    "            device=x_edge_m_0.device,\n",
    "        )\n",
    "        x_edge_m_all = torch.cat((x_edge_m_0, x_edge_m_pad), dim=1)\n",
    "\n",
    "        # Create SO3 embedding\n",
    "        x_edge_embedding = SO3_Embedding(\n",
    "            0,\n",
    "            self.lmax_list.copy(),\n",
    "            self.sphere_channels,\n",
    "            device=x_edge_m_all.device,\n",
    "            dtype=x_edge_m_all.dtype,\n",
    "        )\n",
    "        x_edge_embedding.set_embedding(x_edge_m_all)\n",
    "        x_edge_embedding.set_lmax_mmax(self.lmax_list.copy(), self.mmax_list.copy())\n",
    "\n",
    "        # Reshape the spherical harmonics based on l (degree)\n",
    "        x_edge_embedding._l_primary(self.mappingReduced)\n",
    "\n",
    "        # Rotate back the irreps\n",
    "        x_edge_embedding._rotate_inv(self.SO3_rotation, self.mappingReduced)\n",
    "\n",
    "        # Compute the sum of the incoming neighboring messages for each target node\n",
    "        x_edge_embedding._reduce_edge(edge_index[:,1] - node_offset, num_nodes)\n",
    "        x_edge_embedding.embedding = x_edge_embedding.embedding / self.rescale_factor\n",
    "\n",
    "        return x_edge_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d6dac613",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_deg_embedder = EdgeDegreeEmbedding(\n",
    "    sphere_channels=NUM_CHANNELS,\n",
    "    lmax_list=LMAX_LIST,\n",
    "    mmax_list=MMAX_LIST,\n",
    "    SO3_rotation=SO3_rotation,\n",
    "    mappingReduced=mappingReduced,\n",
    "    radial_basis_size=R.shape[1],\n",
    "    feature_vocab_sizes=feature_vocab_sizes,\n",
    "    use_edge_features=True,\n",
    "    bond_features=collator.featurizer.bond_features,\n",
    "    use_node_features=True,\n",
    "    node_features=collator.featurizer.atom_features,\n",
    "    embedding_dim=32,\n",
    "    embedding_use_bias=True,\n",
    "    projector_hidden_layers=1,\n",
    "    projector_size=64,\n",
    "    rescale_factor=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8b492d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_a_edges = edge_deg_embedder(\n",
    "    edge_distance_rbf=R,\n",
    "    edge_index=batch.edge_index,\n",
    "    num_nodes=batch.positions.shape[0],\n",
    "    feature_dict=feature_dict,\n",
    "    node_offset=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6f543b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_a.embedding = h_a.embedding + h_a_edges.embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31e4899",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_norm_1 = get_normalization_layer(\n",
    "    'layer_norm_sh',\n",
    "    lmax = max(LMAX_LIST),\n",
    "    num_channels=SPHERE_CHANNELS_ALL\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "726623df",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_a.embedding = input_norm_1(h_a.embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6497ee",
   "metadata": {},
   "source": [
    "#### Mixing to create final SO3 embeddings before attention layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "334a5ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from typing import List, Optional\n",
    "\n",
    "class SO3_L1_Linear(nn.Module):\n",
    "    \"\"\"\n",
    "    Equivariant linear layer that operates only on L=1 spherical harmonic features.\n",
    "    Maintains SO(3) equivariance by using shared weights for all m components.\n",
    "    \n",
    "    Input: [N, 3, in_channels] -> Output: [N, 3, out_channels]\n",
    "    \n",
    "    Args:\n",
    "        in_channels: Input feature channels\n",
    "        out_channels: Output feature channels  \n",
    "        bias: Whether to use bias term (should be False for L=1 to maintain equivariance)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int, bias: bool = False):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        # Single weight matrix shared by all m components of L=1\n",
    "        self.weight = nn.Parameter(torch.randn(out_channels, in_channels))\n",
    "        bound = 1 / math.sqrt(in_channels)\n",
    "        nn.init.uniform_(self.weight, -bound, bound)\n",
    "        \n",
    "        if bias:\n",
    "            raise ValueError(\"Bias should be False for L=1 to maintain equivariance\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [N, 3, in_channels] L=1 features (m=-1,0,1 components)\n",
    "        Returns:\n",
    "            [N, 3, out_channels] transformed L=1 features\n",
    "        \"\"\"\n",
    "        # Apply same linear transformation to all m components\n",
    "        # x @ weight.T maintains equivariance since all m share same weights\n",
    "        return torch.einsum('nmi, oi -> nmo', x, self.weight)\n",
    "    \n",
    "\n",
    "class SO3_L1_LinearMixing(nn.Module):\n",
    "    \"\"\"\n",
    "    Applies a linear transformation to L=1 spherical harmonic features.\n",
    "    \n",
    "    This layer is designed to mix L=1 features while maintaining equivariance.\n",
    "    \n",
    "    Args:\n",
    "        in_channels_list: List of input sizes for each item to be mixed\n",
    "        out_channels: Output size for the mixed features\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels_list: List[int], out_channels: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels_list = in_channels_list\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        # Create a linear layer for each input channel size\n",
    "        self.linears = nn.ModuleList([\n",
    "            SO3_L1_Linear(in_channels, out_channels) for in_channels in in_channels_list\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x_list: List[torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x_list: List of tensors with shape [N, 3, in_channels_i] for each input\n",
    "        Returns:\n",
    "            Tensor with shape [N, 3, out_channels] after mixing\n",
    "        \"\"\"\n",
    "        mixed_features = [linear(x) for linear, x in zip(self.linears, x_list)]\n",
    "        return torch.stack(mixed_features, dim=2).sum(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a09977c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mixer = SO3_L1_LinearMixing(\n",
    "    in_channels_list=[SPHERE_CHANNELS_ALL, 3],\n",
    "    out_channels=SPHERE_CHANNELS_ALL\n",
    ")\n",
    "current_l1_features = h_a.embedding[:, 1:4, :]  # Extract L=1 features (m=-1,0,1)\n",
    "\n",
    "mixed_features = mixer([current_l1_features, node_l1_gradients])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "83cbfcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_a.embedding[:, 1:4, :] = mixed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "0dbce236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric\n",
    "\n",
    "from fairchem.core.common import gp_utils\n",
    "\n",
    "from fairchem.core.models.equiformer_v2.activation import (\n",
    "    GateActivation,\n",
    "    S2Activation,\n",
    "    SeparableS2Activation,\n",
    "    SmoothLeakyReLU,\n",
    ")\n",
    "from fairchem.core.models.equiformer_v2.drop import EquivariantDropoutArraySphericalHarmonics, GraphDropPath\n",
    "from fairchem.core.models.equiformer_v2.layer_norm import get_normalization_layer\n",
    "from fairchem.core.models.equiformer_v2.radial_function import RadialFunction\n",
    "from fairchem.core.models.equiformer_v2.so2_ops import SO2_Convolution\n",
    "from fairchem.core.models.equiformer_v2.so3 import SO3_Embedding, SO3_LinearV2\n",
    "\n",
    "\n",
    "class SO2EquivariantGraphAttentionV2(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    SO2EquivariantGraphAttention: Perform MLP attention + non-linear message passing\n",
    "        SO(2) Convolution with radial function -> S2 Activation -> SO(2) Convolution -> attention weights and non-linear messages\n",
    "        attention weights * non-linear messages -> Linear\n",
    "\n",
    "    Args:\n",
    "        sphere_channels (int):      Number of spherical channels\n",
    "        hidden_channels (int):      Number of hidden channels used during the SO(2) conv\n",
    "        num_heads (int):            Number of attention heads\n",
    "        attn_alpha_channels (int):  Number of channels for alpha vector in each attention head\n",
    "        attn_value_channels (int):  Number of channels for value vector in each attention head\n",
    "        output_channels (int):      Number of output channels\n",
    "        lmax_list (list:int):       List of degrees (l) for each resolution\n",
    "        mmax_list (list:int):       List of orders (m) for each resolution\n",
    "\n",
    "        SO3_rotation (list:SO3_Rotation): Class to calculate Wigner-D matrices and rotate embeddings\n",
    "        mappingReduced (CoefficientMappingModule): Class to convert l and m indices once node embedding is rotated\n",
    "        SO3_grid (SO3_grid):        Class used to convert from grid the spherical harmonic representations\n",
    "\n",
    "        edge_channels_list (list:int): List of sizes of invariant edge embedding. For example, [input_channels, hidden_channels, hidden_channels].\n",
    "        use_m_share_rad (bool):     Whether all m components within a type-L vector of one channel share radial function weights\n",
    "\n",
    "        # for EdgeProjector - replaces use_atom_edge_embedding and related parameters\n",
    "        use_edge_information (bool): Whether to use edge information in the attention mechanism\n",
    "        radial_basis_size (int):     Number of radial basis functions expected\n",
    "        feature_vocab_sizes (Dict[str, int]): Dictionary mapping feature names to vocab sizes\n",
    "        use_edge_features (bool):    Whether to use edge features\n",
    "        bond_features (List[str]):   List of bond feature names to use if using any\n",
    "        use_node_features (bool):    Whether to use node features\n",
    "        node_features (List[str]):   List of node feature names to use if using any\n",
    "        embedding_dim (int):         Embedding dimension for node and edge features\n",
    "        embedding_use_bias (bool):   Whether to use bias in the embedding layers\n",
    "\n",
    "        activation (str):           Type of activation function\n",
    "        use_s2_act_attn (bool):     Whether to use attention after S2 activation. Otherwise, use the same attention as Equiformer\n",
    "        use_attn_renorm (bool):     Whether to re-normalize attention weights\n",
    "        use_gate_act (bool):        If `True`, use gate activation. Otherwise, use S2 activation.\n",
    "        use_sep_s2_act (bool):      If `True`, use separable S2 activation when `use_gate_act` is False.\n",
    "\n",
    "        alpha_drop (float):         Dropout rate for attention weights\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sphere_channels: int,\n",
    "        hidden_channels: int,\n",
    "        num_heads: int,\n",
    "        attn_alpha_channels: int,\n",
    "        attn_value_channels: int,\n",
    "        output_channels: int,\n",
    "        lmax_list: list[int],\n",
    "        mmax_list: list[int],\n",
    "        SO3_rotation,\n",
    "        mappingReduced,\n",
    "        SO3_grid,\n",
    "        edge_channels_list,\n",
    "        use_m_share_rad: bool = False,\n",
    "        # EdgeProjector parameters\n",
    "        use_edge_information: bool = True,\n",
    "        radial_basis_size: int = 50,\n",
    "        feature_vocab_sizes: Dict[str, int] = None,\n",
    "        use_edge_features: bool = True,\n",
    "        bond_features: List[str] = None,\n",
    "        use_node_features: bool = True,\n",
    "        node_features: List[str] = None,\n",
    "        embedding_dim: int = 32,\n",
    "        embedding_use_bias: bool = True,\n",
    "        activation=\"scaled_silu\",\n",
    "        use_s2_act_attn: bool = False,\n",
    "        use_attn_renorm: bool = True,\n",
    "        use_gate_act: bool = False,\n",
    "        use_sep_s2_act: bool = True,\n",
    "        alpha_drop: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.sphere_channels = sphere_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.num_heads = num_heads\n",
    "        self.attn_alpha_channels = attn_alpha_channels\n",
    "        self.attn_value_channels = attn_value_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.lmax_list = lmax_list\n",
    "        self.mmax_list = mmax_list\n",
    "        self.num_resolutions = len(self.lmax_list)\n",
    "\n",
    "        self.SO3_rotation = SO3_rotation\n",
    "        self.mappingReduced = mappingReduced\n",
    "        self.SO3_grid = SO3_grid\n",
    "\n",
    "        # Edge feature processing\n",
    "        self.use_edge_information = use_edge_information\n",
    "        self.use_m_share_rad = use_m_share_rad\n",
    "        \n",
    "        if feature_vocab_sizes is None:\n",
    "            feature_vocab_sizes = {}\n",
    "        if bond_features is None:\n",
    "            bond_features = ['bond_order', 'is_in_ring', 'is_aromatic']\n",
    "        if node_features is None:\n",
    "            node_features = ['element', 'charge', 'nhyd', 'hyb']\n",
    "\n",
    "        # Initialize edge projector\n",
    "        if self.use_edge_information:\n",
    "            self.edge_projector = EdgeProjector(\n",
    "                radial_basis_size=radial_basis_size,\n",
    "                feature_vocab_sizes=feature_vocab_sizes,\n",
    "                use_edge_features=use_edge_features,\n",
    "                bond_features=bond_features,\n",
    "                use_node_features=use_node_features,\n",
    "                node_features=node_features,\n",
    "                output_dim=edge_channels_list[-1],  # Match expected output dimension\n",
    "                embedding_dim=embedding_dim,\n",
    "                embedding_use_bias=embedding_use_bias,\n",
    "                use_projector=False,  # Just concatenation, no radial function\n",
    "                projector_hidden_layers=1,\n",
    "                projector_size=64\n",
    "            )\n",
    "            # Update edge channels list input size based on projector concatenated size\n",
    "            self.edge_channels_list = copy.deepcopy(edge_channels_list)\n",
    "            self.edge_channels_list[0] = self.edge_projector.input_size\n",
    "        else:\n",
    "            self.edge_projector = None\n",
    "            self.edge_channels_list = copy.deepcopy(edge_channels_list)\n",
    "\n",
    "        self.use_s2_act_attn = use_s2_act_attn\n",
    "        self.use_attn_renorm = use_attn_renorm\n",
    "        self.use_gate_act = use_gate_act\n",
    "        self.use_sep_s2_act = use_sep_s2_act\n",
    "\n",
    "        assert not self.use_s2_act_attn  # since this is not used\n",
    "\n",
    "        # Create SO(2) convolution blocks\n",
    "        extra_m0_output_channels = None\n",
    "        if not self.use_s2_act_attn:\n",
    "            extra_m0_output_channels = self.num_heads * self.attn_alpha_channels\n",
    "            if self.use_gate_act:\n",
    "                extra_m0_output_channels = (\n",
    "                    extra_m0_output_channels\n",
    "                    + max(self.lmax_list) * self.hidden_channels\n",
    "                )\n",
    "            else:\n",
    "                if self.use_sep_s2_act:\n",
    "                    extra_m0_output_channels = (\n",
    "                        extra_m0_output_channels + self.hidden_channels\n",
    "                    )\n",
    "\n",
    "        if self.use_m_share_rad:\n",
    "            self.edge_channels_list = [\n",
    "                *self.edge_channels_list,\n",
    "                2 * self.sphere_channels * (max(self.lmax_list) + 1),\n",
    "            ]\n",
    "            self.rad_func = RadialFunction(self.edge_channels_list)\n",
    "            expand_index = torch.zeros([(max(self.lmax_list) + 1) ** 2]).long()\n",
    "            for lval in range(max(self.lmax_list) + 1):\n",
    "                start_idx = lval**2\n",
    "                length = 2 * lval + 1\n",
    "                expand_index[start_idx : (start_idx + length)] = lval\n",
    "            self.register_buffer(\"expand_index\", expand_index)\n",
    "\n",
    "        self.so2_conv_1 = SO2_Convolution(\n",
    "            2 * self.sphere_channels,\n",
    "            self.hidden_channels,\n",
    "            self.lmax_list,\n",
    "            self.mmax_list,\n",
    "            self.mappingReduced,\n",
    "            internal_weights=(bool(self.use_m_share_rad)),\n",
    "            edge_channels_list=(\n",
    "                self.edge_channels_list if not self.use_m_share_rad else None\n",
    "            ),\n",
    "            extra_m0_output_channels=extra_m0_output_channels,  # for attention weights and/or gate activation\n",
    "        )\n",
    "\n",
    "        if self.use_s2_act_attn:\n",
    "            self.alpha_norm = None\n",
    "            self.alpha_act = None\n",
    "            self.alpha_dot = None\n",
    "        else:\n",
    "            if self.use_attn_renorm:\n",
    "                self.alpha_norm = torch.nn.LayerNorm(self.attn_alpha_channels)\n",
    "            else:\n",
    "                self.alpha_norm = torch.nn.Identity()\n",
    "            self.alpha_act = SmoothLeakyReLU()\n",
    "            self.alpha_dot = torch.nn.Parameter(\n",
    "                torch.randn(self.num_heads, self.attn_alpha_channels)\n",
    "            )\n",
    "            # torch_geometric.nn.inits.glorot(self.alpha_dot) # Following GATv2\n",
    "            std = 1.0 / math.sqrt(self.attn_alpha_channels)\n",
    "            torch.nn.init.uniform_(self.alpha_dot, -std, std)\n",
    "\n",
    "        self.alpha_dropout = None\n",
    "        if alpha_drop != 0.0:\n",
    "            self.alpha_dropout = torch.nn.Dropout(alpha_drop)\n",
    "\n",
    "        if self.use_gate_act:\n",
    "            self.gate_act = GateActivation(\n",
    "                lmax=max(self.lmax_list),\n",
    "                mmax=max(self.mmax_list),\n",
    "                num_channels=self.hidden_channels,\n",
    "            )\n",
    "        else:\n",
    "            if self.use_sep_s2_act:\n",
    "                # separable S2 activation\n",
    "                self.s2_act = SeparableS2Activation(\n",
    "                    lmax=max(self.lmax_list), mmax=max(self.mmax_list)\n",
    "                )\n",
    "            else:\n",
    "                # S2 activation\n",
    "                self.s2_act = S2Activation(\n",
    "                    lmax=max(self.lmax_list), mmax=max(self.mmax_list)\n",
    "                )\n",
    "\n",
    "        self.so2_conv_2 = SO2_Convolution(\n",
    "            self.hidden_channels,\n",
    "            self.num_heads * self.attn_value_channels,\n",
    "            self.lmax_list,\n",
    "            self.mmax_list,\n",
    "            self.mappingReduced,\n",
    "            internal_weights=True,\n",
    "            edge_channels_list=None,\n",
    "            extra_m0_output_channels=(\n",
    "                self.num_heads if self.use_s2_act_attn else None\n",
    "            ),  # for attention weights\n",
    "        )\n",
    "\n",
    "        self.proj = SO3_LinearV2(\n",
    "            self.num_heads * self.attn_value_channels,\n",
    "            self.output_channels,\n",
    "            lmax=self.lmax_list[0],\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        edge_distance: torch.Tensor,\n",
    "        edge_index,\n",
    "        feature_dict: Dict[str, torch.Tensor] = None,\n",
    "        node_offset: int = 0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Forward pass through SO2EquivariantGraphAttention.\n",
    "        \n",
    "        Args:\n",
    "            x: SO3_Embedding node features\n",
    "            edge_distance: [E, radial_basis_size] radial basis encoded distances\n",
    "            edge_index: [2, E] edge connectivity\n",
    "            feature_dict: Dictionary of additional node/edge features for EdgeProjector\n",
    "            node_offset: Node offset for distributed computing\n",
    "            \n",
    "        Returns:\n",
    "            SO3_Embedding: Updated node embeddings\n",
    "        \"\"\"\n",
    "        # Compute edge scalar features (invariant to rotations)\n",
    "        if self.use_edge_information:\n",
    "            if feature_dict is None:\n",
    "                feature_dict = {}\n",
    "            x_edge = self.edge_projector(edge_distance, edge_index, feature_dict)\n",
    "        else:\n",
    "            x_edge = edge_distance\n",
    "\n",
    "        x_source = x.clone()\n",
    "        x_target = x.clone()\n",
    "        if gp_utils.initialized():\n",
    "            x_full = gp_utils.gather_from_model_parallel_region(x.embedding, dim=0)\n",
    "            x_source.set_embedding(x_full)\n",
    "            x_target.set_embedding(x_full)\n",
    "        x_source._expand_edge(edge_index[:,0])\n",
    "        x_target._expand_edge(edge_index[:,1])\n",
    "\n",
    "        x_message_data = torch.cat((x_source.embedding, x_target.embedding), dim=2)\n",
    "        x_message = SO3_Embedding(\n",
    "            0,\n",
    "            x_target.lmax_list.copy(),\n",
    "            x_target.num_channels * 2,\n",
    "            device=x_target.device,\n",
    "            dtype=x_target.dtype,\n",
    "        )\n",
    "        x_message.set_embedding(x_message_data)\n",
    "        x_message.set_lmax_mmax(self.lmax_list.copy(), self.mmax_list.copy())\n",
    "\n",
    "        # radial function (scale all m components within a type-L vector of one channel with the same weight)\n",
    "        if self.use_m_share_rad:\n",
    "            x_edge_weight = self.rad_func(x_edge)\n",
    "            x_edge_weight = x_edge_weight.reshape(\n",
    "                -1, (max(self.lmax_list) + 1), 2 * self.sphere_channels\n",
    "            )\n",
    "            x_edge_weight = torch.index_select(\n",
    "                x_edge_weight, dim=1, index=self.expand_index\n",
    "            )  # [E, (L_max + 1) ** 2, C]\n",
    "            x_message.embedding = x_message.embedding * x_edge_weight\n",
    "\n",
    "        # Rotate the irreps to align with the edge\n",
    "        x_message._rotate(self.SO3_rotation, self.lmax_list, self.mmax_list)\n",
    "\n",
    "        # First SO(2)-convolution\n",
    "        if self.use_s2_act_attn:\n",
    "            x_message = self.so2_conv_1(x_message, x_edge)\n",
    "        else:\n",
    "            x_message, x_0_extra = self.so2_conv_1(x_message, x_edge)\n",
    "\n",
    "        # Activation\n",
    "        x_alpha_num_channels = self.num_heads * self.attn_alpha_channels\n",
    "        if self.use_gate_act:\n",
    "            # Gate activation\n",
    "            x_0_gating = x_0_extra.narrow(\n",
    "                1,\n",
    "                x_alpha_num_channels,\n",
    "                x_0_extra.shape[1] - x_alpha_num_channels,\n",
    "            )  # for activation\n",
    "            x_0_alpha = x_0_extra.narrow(\n",
    "                1, 0, x_alpha_num_channels\n",
    "            )  # for attention weights\n",
    "            x_message.embedding = self.gate_act(x_0_gating, x_message.embedding)\n",
    "        else:\n",
    "            if self.use_sep_s2_act:\n",
    "                x_0_gating = x_0_extra.narrow(\n",
    "                    1,\n",
    "                    x_alpha_num_channels,\n",
    "                    x_0_extra.shape[1] - x_alpha_num_channels,\n",
    "                )  # for activation\n",
    "                x_0_alpha = x_0_extra.narrow(\n",
    "                    1, 0, x_alpha_num_channels\n",
    "                )  # for attention weights\n",
    "                x_message.embedding = self.s2_act(\n",
    "                    x_0_gating, x_message.embedding, self.SO3_grid\n",
    "                )\n",
    "            else:\n",
    "                x_0_alpha = x_0_extra\n",
    "                x_message.embedding = self.s2_act(x_message.embedding, self.SO3_grid)\n",
    "\n",
    "        # Second SO(2)-convolution\n",
    "        if self.use_s2_act_attn:\n",
    "            x_message, x_0_extra = self.so2_conv_2(x_message, x_edge)\n",
    "        else:\n",
    "            x_message = self.so2_conv_2(x_message, x_edge)\n",
    "\n",
    "        # Attention weights\n",
    "        if self.use_s2_act_attn:\n",
    "            alpha = x_0_extra\n",
    "        else:\n",
    "            x_0_alpha = x_0_alpha.reshape(-1, self.num_heads, self.attn_alpha_channels)\n",
    "            x_0_alpha = self.alpha_norm(x_0_alpha)\n",
    "            x_0_alpha = self.alpha_act(x_0_alpha)\n",
    "            alpha = torch.einsum(\"bik, ik -> bi\", x_0_alpha, self.alpha_dot)\n",
    "        alpha = torch_geometric.utils.softmax(alpha, edge_index[:,1])\n",
    "        alpha = alpha.reshape(alpha.shape[0], 1, self.num_heads, 1)\n",
    "        if self.alpha_dropout is not None:\n",
    "            alpha = self.alpha_dropout(alpha)\n",
    "\n",
    "        # Attention weights * non-linear messages\n",
    "        attn = x_message.embedding\n",
    "        attn = attn.reshape(\n",
    "            attn.shape[0],\n",
    "            attn.shape[1],\n",
    "            self.num_heads,\n",
    "            self.attn_value_channels,\n",
    "        )\n",
    "        attn = attn * alpha\n",
    "        attn = attn.reshape(\n",
    "            attn.shape[0],\n",
    "            attn.shape[1],\n",
    "            self.num_heads * self.attn_value_channels,\n",
    "        )\n",
    "        x_message.embedding = attn\n",
    "\n",
    "        # Rotate back the irreps\n",
    "        x_message._rotate_inv(self.SO3_rotation, self.mappingReduced)\n",
    "\n",
    "        # Compute the sum of the incoming neighboring messages for each target node\n",
    "        x_message._reduce_edge(edge_index[:,1] - node_offset, len(x.embedding))\n",
    "\n",
    "        # Project\n",
    "        return self.proj(x_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "2aae0e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = SO2EquivariantGraphAttentionV2(\n",
    "    sphere_channels=NUM_CHANNELS,\n",
    "    hidden_channels=NUM_CHANNELS,\n",
    "    num_heads=4,\n",
    "    attn_alpha_channels=NUM_CHANNELS,\n",
    "    attn_value_channels=NUM_CHANNELS,\n",
    "    output_channels=NUM_CHANNELS,\n",
    "    lmax_list=LMAX_LIST,\n",
    "    mmax_list=MMAX_LIST,\n",
    "    SO3_rotation=SO3_rotation,\n",
    "    mappingReduced=mappingReduced,\n",
    "    SO3_grid=SO3_grid,\n",
    "    edge_channels_list=[R.shape[1], NUM_CHANNELS, NUM_CHANNELS],\n",
    "    use_m_share_rad=False,\n",
    "    use_edge_information=True,\n",
    "    radial_basis_size=R.shape[1],\n",
    "    feature_vocab_sizes=feature_vocab_sizes,\n",
    "    use_edge_features=True,\n",
    "    bond_features=collator.featurizer.bond_features,\n",
    "    use_node_features=True,\n",
    "    node_features=collator.featurizer.atom_features,\n",
    "    embedding_dim=32,\n",
    "    embedding_use_bias=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "a748f3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = attn(\n",
    "    x=h_a,\n",
    "    edge_distance=R,\n",
    "    edge_index=batch.edge_index,\n",
    "    feature_dict=feature_dict,\n",
    "    node_offset=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "9c0abb9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fairchem.core.models.equiformer_v2.so3.SO3_Embedding at 0x7fc66d26b220>"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2148d63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metal3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
